{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ada9d64",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658f33c6",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cbe366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "175b12a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.12.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e8c014",
   "metadata": {},
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3494fa44",
   "metadata": {},
   "source": [
    "### Preprocessing the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba91d132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4782 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# data augumentation is need, it will tilt or twist the images.\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'flower_images/training_set', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'flower_images/training_set',  # this is the target directory\n",
    "        target_size=(64, 64),  # all images will be resized to 150x150\n",
    "        batch_size = 32,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef8220c",
   "metadata": {},
   "source": [
    "### Preprocessing the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "210afc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a similar generator, for validation data  or test_set\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        'flower_images/test_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5420ab",
   "metadata": {},
   "source": [
    "## Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c3c0a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a656d0ad",
   "metadata": {},
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8388f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3814617",
   "metadata": {},
   "source": [
    "### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b2e0ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a5639c",
   "metadata": {},
   "source": [
    "### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63626676",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb94fd7",
   "metadata": {},
   "source": [
    "### Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1777cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6262f45a",
   "metadata": {},
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9b103e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079fa072",
   "metadata": {},
   "source": [
    "### Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b76cb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Dense(units=200, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490c66be",
   "metadata": {},
   "source": [
    "### Step 5 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fe63d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Dense(units=5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c9c777",
   "metadata": {},
   "source": [
    "## Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f1b2d2",
   "metadata": {},
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8514cb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba28ccd",
   "metadata": {},
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f91f8262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "150/150 [==============================] - 128s 842ms/step - loss: 1.2418 - accuracy: 0.5002 - val_loss: 1.0140 - val_accuracy: 0.5800\n",
      "Epoch 2/25\n",
      "150/150 [==============================] - 79s 529ms/step - loss: 1.0163 - accuracy: 0.6025 - val_loss: 0.8870 - val_accuracy: 0.6400\n",
      "Epoch 3/25\n",
      "150/150 [==============================] - 79s 524ms/step - loss: 0.8804 - accuracy: 0.6541 - val_loss: 0.7758 - val_accuracy: 0.6880\n",
      "Epoch 4/25\n",
      "150/150 [==============================] - 78s 523ms/step - loss: 0.7868 - accuracy: 0.6913 - val_loss: 0.6537 - val_accuracy: 0.7440\n",
      "Epoch 5/25\n",
      "150/150 [==============================] - 79s 530ms/step - loss: 0.7299 - accuracy: 0.7263 - val_loss: 0.5752 - val_accuracy: 0.7740\n",
      "Epoch 6/25\n",
      "150/150 [==============================] - 79s 527ms/step - loss: 0.6497 - accuracy: 0.7578 - val_loss: 0.4698 - val_accuracy: 0.8400\n",
      "Epoch 7/25\n",
      "150/150 [==============================] - 50s 330ms/step - loss: 0.5958 - accuracy: 0.7737 - val_loss: 0.4169 - val_accuracy: 0.8580\n",
      "Epoch 8/25\n",
      "150/150 [==============================] - 50s 330ms/step - loss: 0.4947 - accuracy: 0.8170 - val_loss: 0.3312 - val_accuracy: 0.8760\n",
      "Epoch 9/25\n",
      "150/150 [==============================] - 55s 365ms/step - loss: 0.4603 - accuracy: 0.8275 - val_loss: 0.4208 - val_accuracy: 0.8500\n",
      "Epoch 10/25\n",
      "150/150 [==============================] - 76s 505ms/step - loss: 0.4169 - accuracy: 0.8484 - val_loss: 0.2700 - val_accuracy: 0.9020\n",
      "Epoch 11/25\n",
      "150/150 [==============================] - 50s 331ms/step - loss: 0.3623 - accuracy: 0.8685 - val_loss: 0.2246 - val_accuracy: 0.9220\n",
      "Epoch 12/25\n",
      "150/150 [==============================] - 50s 334ms/step - loss: 0.3049 - accuracy: 0.8867 - val_loss: 0.2345 - val_accuracy: 0.9180\n",
      "Epoch 13/25\n",
      "150/150 [==============================] - 50s 331ms/step - loss: 0.2991 - accuracy: 0.8929 - val_loss: 0.1978 - val_accuracy: 0.9340\n",
      "Epoch 14/25\n",
      "150/150 [==============================] - 50s 331ms/step - loss: 0.2481 - accuracy: 0.9107 - val_loss: 0.1341 - val_accuracy: 0.9500\n",
      "Epoch 15/25\n",
      "150/150 [==============================] - 57s 379ms/step - loss: 0.2261 - accuracy: 0.9212 - val_loss: 0.1718 - val_accuracy: 0.9340\n",
      "Epoch 16/25\n",
      "150/150 [==============================] - 51s 342ms/step - loss: 0.2211 - accuracy: 0.9251 - val_loss: 0.1197 - val_accuracy: 0.9680\n",
      "Epoch 17/25\n",
      "150/150 [==============================] - 49s 327ms/step - loss: 0.1821 - accuracy: 0.9325 - val_loss: 0.1155 - val_accuracy: 0.9640\n",
      "Epoch 18/25\n",
      "150/150 [==============================] - 52s 345ms/step - loss: 0.1681 - accuracy: 0.9404 - val_loss: 0.1767 - val_accuracy: 0.9480\n",
      "Epoch 19/25\n",
      "150/150 [==============================] - 50s 336ms/step - loss: 0.1548 - accuracy: 0.9486 - val_loss: 0.0944 - val_accuracy: 0.9720\n",
      "Epoch 20/25\n",
      "150/150 [==============================] - 50s 330ms/step - loss: 0.1506 - accuracy: 0.9465 - val_loss: 0.0917 - val_accuracy: 0.9740\n",
      "Epoch 21/25\n",
      "150/150 [==============================] - 49s 327ms/step - loss: 0.1107 - accuracy: 0.9615 - val_loss: 0.0930 - val_accuracy: 0.9640\n",
      "Epoch 22/25\n",
      "150/150 [==============================] - 49s 328ms/step - loss: 0.1202 - accuracy: 0.9598 - val_loss: 0.0824 - val_accuracy: 0.9780\n",
      "Epoch 23/25\n",
      "150/150 [==============================] - 49s 326ms/step - loss: 0.1187 - accuracy: 0.9607 - val_loss: 0.0499 - val_accuracy: 0.9840\n",
      "Epoch 24/25\n",
      "150/150 [==============================] - 49s 327ms/step - loss: 0.1388 - accuracy: 0.9561 - val_loss: 0.0860 - val_accuracy: 0.9780\n",
      "Epoch 25/25\n",
      "150/150 [==============================] - 49s 327ms/step - loss: 0.1129 - accuracy: 0.9617 - val_loss: 0.0703 - val_accuracy: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ec44a1f400>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=training_set, validation_data=test_set, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf920f0",
   "metadata": {},
   "source": [
    "## Part 4 - Making a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bedfd526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "Tulip\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "import numpy as np\n",
    "\n",
    "test_image = load_img(\"flower_images/test_set/Tulip/0d14d52dcb.jpg\", target_size=(64,64))\n",
    "\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image,axis=0)\n",
    "\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "\n",
    "if result[0,0] == 1:\n",
    "    prediction = \"Lilly\"\n",
    "elif result[0,1] == 1:\n",
    "    prediction = \"Lotus\"\n",
    "elif result[0,2] == 1:\n",
    "    prediction = \"Orchid\"\n",
    "elif result[0,3] == 1:\n",
    "    prediction = \"Sunflower\"\n",
    "else:\n",
    "    prediction = \"Tulip\"\n",
    "    \n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dff71f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcddc43f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
