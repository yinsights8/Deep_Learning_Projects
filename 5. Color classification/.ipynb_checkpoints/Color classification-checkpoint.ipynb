{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91bcb8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# importing visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3277653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e940a3",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df8bf007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffb5cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "176ada92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5231 images belonging to 12 classes.\n",
      "Found 1008 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "## 1. Preprocessing the training Data \n",
    "\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'color/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_set = train_datagen.flow_from_directory(\n",
    "        'Color_classificatoin/train_set',  # this is the target directory\n",
    "        target_size=(224, 224),  # all images will be resized to 224X224\n",
    "        color_mode = 'rgb',\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1. Preprocessing the Test Data \n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "# target_size=(150, 150)\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        'Color_classificatoin/test_set',\n",
    "        target_size=(224, 224),\n",
    "        color_mode = 'rgb',\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06f325e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train_set.class_indices.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a601ed11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['black', 'blue', 'brown', 'green', 'grey', 'orange', 'pink', 'purple', 'red', 'silver', 'white', 'yellow'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b575708",
   "metadata": {},
   "source": [
    "### Building CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04b7f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing cnn libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94236d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing model\n",
    "cnn = Sequential()\n",
    "\n",
    "# 1. convolutional layer\n",
    "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[224, 224,3]))\n",
    "\n",
    "# 2. pooling\n",
    "cnn.add(MaxPool2D(pool_size=2,strides=2,padding='valid'))\n",
    "\n",
    "# 3. adding a second layer of convolutional layer\n",
    "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(MaxPool2D(pool_size=2,strides=2,padding='valid'))\n",
    "\n",
    "# 4. Flatten \n",
    "cnn.add(Flatten())\n",
    "\n",
    "# 5. adding ann layer\n",
    "cnn.add(Dense(units=200, activation='relu',))\n",
    "cnn.add(Dense(units=12, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1bef8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling cnn model\n",
    "cnn.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b8435ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 111, 111, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 109, 109, 32)      9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 54, 54, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 93312)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200)               18662600  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12)                2412      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,675,156\n",
      "Trainable params: 18,675,156\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# summary\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5763430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "164/164 [==============================] - 329s 2s/step - loss: 1.9226 - accuracy: 0.4840 - val_loss: 1.6708 - val_accuracy: 0.4702\n",
      "Epoch 2/25\n",
      "164/164 [==============================] - 234s 1s/step - loss: 1.0165 - accuracy: 0.6622 - val_loss: 1.4391 - val_accuracy: 0.5337\n",
      "Epoch 3/25\n",
      "164/164 [==============================] - 232s 1s/step - loss: 0.8812 - accuracy: 0.7064 - val_loss: 1.5860 - val_accuracy: 0.5050\n",
      "Epoch 4/25\n",
      "164/164 [==============================] - 233s 1s/step - loss: 0.8046 - accuracy: 0.7352 - val_loss: 1.8382 - val_accuracy: 0.4821\n",
      "Epoch 5/25\n",
      "164/164 [==============================] - 231s 1s/step - loss: 0.7264 - accuracy: 0.7603 - val_loss: 1.3636 - val_accuracy: 0.5655\n",
      "Epoch 6/25\n",
      "164/164 [==============================] - 229s 1s/step - loss: 0.7080 - accuracy: 0.7649 - val_loss: 1.6306 - val_accuracy: 0.5377\n",
      "Epoch 7/25\n",
      "164/164 [==============================] - 230s 1s/step - loss: 0.6821 - accuracy: 0.7662 - val_loss: 1.3413 - val_accuracy: 0.5556\n",
      "Epoch 8/25\n",
      "164/164 [==============================] - 220s 1s/step - loss: 0.6032 - accuracy: 0.7956 - val_loss: 1.6063 - val_accuracy: 0.5556\n",
      "Epoch 9/25\n",
      "164/164 [==============================] - 180s 1s/step - loss: 0.5763 - accuracy: 0.8031 - val_loss: 1.6221 - val_accuracy: 0.5655\n",
      "Epoch 10/25\n",
      "164/164 [==============================] - 163s 990ms/step - loss: 0.5386 - accuracy: 0.8195 - val_loss: 1.4050 - val_accuracy: 0.5923\n",
      "Epoch 11/25\n",
      "164/164 [==============================] - 166s 1s/step - loss: 0.5614 - accuracy: 0.8155 - val_loss: 1.4189 - val_accuracy: 0.6062\n",
      "Epoch 12/25\n",
      "164/164 [==============================] - 170s 1s/step - loss: 0.5359 - accuracy: 0.8203 - val_loss: 1.6226 - val_accuracy: 0.5972\n",
      "Epoch 13/25\n",
      "164/164 [==============================] - 170s 1s/step - loss: 0.4975 - accuracy: 0.8320 - val_loss: 1.6637 - val_accuracy: 0.5635\n",
      "Epoch 14/25\n",
      "164/164 [==============================] - 166s 1s/step - loss: 0.4777 - accuracy: 0.8366 - val_loss: 1.5604 - val_accuracy: 0.5933\n",
      "Epoch 15/25\n",
      "164/164 [==============================] - 565s 3s/step - loss: 0.4732 - accuracy: 0.8390 - val_loss: 1.5569 - val_accuracy: 0.5665\n",
      "Epoch 16/25\n",
      "164/164 [==============================] - 121s 734ms/step - loss: 0.4275 - accuracy: 0.8513 - val_loss: 1.8166 - val_accuracy: 0.5665\n",
      "Epoch 17/25\n",
      "164/164 [==============================] - 118s 721ms/step - loss: 0.4150 - accuracy: 0.8587 - val_loss: 1.9163 - val_accuracy: 0.5724\n",
      "Epoch 18/25\n",
      "164/164 [==============================] - 120s 730ms/step - loss: 0.4131 - accuracy: 0.8566 - val_loss: 1.6760 - val_accuracy: 0.5863\n",
      "Epoch 19/25\n",
      "164/164 [==============================] - 117s 713ms/step - loss: 0.3898 - accuracy: 0.8681 - val_loss: 1.7953 - val_accuracy: 0.5873\n",
      "Epoch 20/25\n",
      "164/164 [==============================] - 117s 712ms/step - loss: 0.4203 - accuracy: 0.8589 - val_loss: 1.8994 - val_accuracy: 0.5903\n",
      "Epoch 21/25\n",
      "164/164 [==============================] - 121s 735ms/step - loss: 0.3842 - accuracy: 0.8748 - val_loss: 1.9836 - val_accuracy: 0.5694\n",
      "Epoch 22/25\n",
      "164/164 [==============================] - 119s 727ms/step - loss: 0.3655 - accuracy: 0.8706 - val_loss: 1.8246 - val_accuracy: 0.5794\n",
      "Epoch 23/25\n",
      "164/164 [==============================] - 119s 725ms/step - loss: 0.3290 - accuracy: 0.8872 - val_loss: 2.4404 - val_accuracy: 0.5278\n",
      "Epoch 24/25\n",
      "164/164 [==============================] - 120s 728ms/step - loss: 0.3519 - accuracy: 0.8807 - val_loss: 2.0732 - val_accuracy: 0.5407\n",
      "Epoch 25/25\n",
      "164/164 [==============================] - 118s 718ms/step - loss: 0.3328 - accuracy: 0.8866 - val_loss: 2.0008 - val_accuracy: 0.5714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1de36775e50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=train_set, validation_data=test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e4aa55",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb66b09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "white\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "import numpy as np\n",
    "\n",
    "test_image = load_img(\"Color_classificatoin/test_set/black/105186615.jpg\", target_size=(224, 224))\n",
    "\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image,axis=0)\n",
    "\n",
    "result = cnn.predict(test_image)\n",
    "train_set.class_indices\n",
    "\n",
    "if result[0,0] == 1:\n",
    "    prediction = \"black\"\n",
    "elif result[0,1] == 1:\n",
    "    prediction = \"blue\"\n",
    "elif result[0,2] == 1:\n",
    "    prediction = \"brown\"\n",
    "elif result[0,3] == 1:\n",
    "    prediction = \"green\"\n",
    "elif result[0,4] == 1:\n",
    "    prediction = \"grey\"\n",
    "elif result[0,5] == 1:\n",
    "    prediction = \"orange\"\n",
    "elif result[0,6] == 1:\n",
    "    prediction = \"pink\"\n",
    "elif result[0,7] == 1:\n",
    "    prediction = \"purple\"\n",
    "elif result[0,8] == 1:\n",
    "    prediction = \"red\"\n",
    "elif result[0,9] == 1:\n",
    "    prediction = \"silver\"\n",
    "elif result[0,10] == 1:\n",
    "    prediction = \"white\"\n",
    "else:\n",
    "    prediction = \"yellow\"\n",
    "    \n",
    "\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85b54c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8f9a8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
